// Module included in the following assemblies:
//
// * installing/installing_azure/installing-azure-user-infra.adoc
// * installing/installing_bare_metal/installing-bare-metal.adoc
// * installing/installing_bare_metal/installing-bare-metal-network-customizations.adoc
// * installing/installing_bare_metal/installing-restricted-networks-bare-metal.adoc
// * installing/installing_gcp/installing-gcp-user-infra.adoc
// * installing/installing_gcp/installing-gcp-user-infra-vpc.adoc
// * installing/installing_gcp/installing-restricted-networks-gcp.adoc
// * installing/installing_platform_agnostic/installing-platform-agnostic.adoc
// * installing/installing_vmc/installing-restricted-networks-vmc-user-infra.adoc
// * installing/installing_vmc/installing-vmc-user-infra.adoc
// * installing/installing_vmc/installing-vmc-network-customizations-user-infra.adoc
// * installing/installing_vsphere/installing-restricted-networks-vsphere.adoc
// * installing/installing_vsphere/installing-vsphere.adoc
// * installing/installing_vsphere/installing-vsphere-network-customizations.adoc
// * installing/installing_ibm_z/installing-ibm-z.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc
// * installing/installing_ibm_z/installing-ibm-z-kvm.adoc
// * installing/installing_ibm_z/installing-ibm-power.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-power.adoc
// * installing/installing-rhv-restricted-network.adoc
// * installing/installing-rhv-user-infra.adoc

ifeval::["{context}" == "installing-vsphere"]
:vsphere:
endif::[]

ifeval::["{context}" == "installing-restricted-networks-vsphere"]
:vsphere:
endif::[]

ifeval::["{context}" == "installing-vsphere-network-customizations"]
:vsphere:
endif::[]

ifeval::["{context}" == "installing-ibm-z"]
:ibm-z:
endif::[]
ifeval::["{context}" == "installing-ibm-z-kvm"]
:ibm-z-kvm:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z"]
:restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-power"]
:restricted:
endif::[]
ifeval::["{context}" == "installing-rhv-user-infra"]
:rhv:
endif::[]
ifeval::["{context}" == "installing-rhv-restricted-network"]
:rhv:
endif::[]

[id="installation-network-user-infra_{context}"]
= Networking requirements for user-provisioned infrastructure

All the {op-system-first} machines require network in `initramfs` during boot
to fetch Ignition config from the machine config server.

ifndef::azure,gcp[]
ifdef::ibm-z[]
During the initial boot, the machines require an HTTP or HTTPS server to
establish a network connection to download their Ignition config files.

Ensure that the machines have persistent IP addresses and host names.
endif::ibm-z[]
ifndef::ibm-z[]
During the initial boot, the machines require either a DHCP server
or that static IP addresses be set on each host in the cluster to
establish a network connection, which allows them to download their Ignition config files.

It is recommended to use the DHCP server to manage the machines for the cluster
long-term. Ensure that the DHCP server is configured to provide persistent IP
addresses and host names to the cluster machines.
endif::ibm-z[]

The Kubernetes API server must be able to resolve the node names of the cluster
machines. If the API servers and worker nodes are in different zones, you can
configure a default DNS search zone to allow the API server to resolve the
node names. Another supported approach is to always refer to hosts by their
fully-qualified domain names in both the node objects and all DNS requests.
endif::azure,gcp[]

You must configure the network connectivity between machines to allow cluster
components to communicate. Each machine must be able to resolve the host names
of all other machines in the cluster.

ifdef::rhv[]
.Firewall

Configure your firewall so your cluster has access to required sites.

See also:

ifndef::openshift-origin[]
* link:https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/planning_and_prerequisites_guide/index#RHV-manager-firewall-requirements_RHV_planning[Red Hat Virtualization Manager firewall requirements]
* link:https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/planning_and_prerequisites_guide#host-firewall-requirements_RHV_planning[Host firewall requirements]
endif::[]
ifdef::openshift-origin[]
* link:https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#RHV-manager-firewall-requirements_SHE_cli_deploy[oVirt Engine firewall requirements]
* link:https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#host-firewall-requirements_SHE_cli_deploy[Host firewall requirements]
endif::[]

ifeval::["{context}" == "installing-rhv-user-infra"]
.Load balancers

Configure one or preferably two layer-4 load balancers:

* Provide load balancing for ports `6443` and `22623` on the control plane and bootstrap machines. Port `6443` provides access to the Kubernetes API server and must be reachable both internally and externally. Port `22623` must be accessible to nodes within the cluster.

* Provide load balancing for port `443` and `80` for machines that run the Ingress router, which are usually compute nodes in the default configuration. Both ports must be accessible from within and outside the cluster.
endif::[]

.DNS

Configure infrastructure-provided DNS to allow the correct resolution of the main components and services. If you use only one load balancer, these DNS records can point to the same IP address.

* Create DNS records for `api.<cluster_name>.<base_domain>` (internal and external resolution) and `api-int.<cluster_name>.<base_domain>` (internal resolution) that point to the load balancer for the control plane machines.

* Create a DNS record for `*.apps.<cluster_name>.<base_domain>` that points to the load balancer for the Ingress router. For example, ports `443` and `80` of the compute machines.
endif::rhv[]

ifeval::["{context}" == "installing-rhv-restricted-network"]
:!rhv:
endif::[]
ifeval::["{context}" == "installing-rhv-user-infra"]
:!rhv:
endif::[]

ifdef::ibm-z-kvm[]
[NOTE]
====
The {op-system-base} KVM host must be configured to use bridged networking in libvirt or MacVTap to connect the network to the virtual machines. The virtual machines must have access to the network, which is attached to the {op-system-base} KVM host. Virtual Networks, for example network address translation (NAT), within KVM are not a supported configuration.
====
endif::ibm-z-kvm[]

.All machines to all machines
[cols="2a,2a,5a",options="header"]
|===

|Protocol
|Port
|Description

|ICMP
|N/A
|Network reachability tests

.4+|TCP
|`1936`
|Metrics

|`9000`-`9999`
|Host level services, including the node exporter on ports `9100`-`9101` and
the Cluster Version Operator on port `9099`.

|`10250`-`10259`
|The default ports that Kubernetes reserves

|`10256`
|openshift-sdn

.3+|UDP
|`4789`
|VXLAN and Geneve

|`6081`
|VXLAN and Geneve

|`9000`-`9999`
|Host level services, including the node exporter on ports `9100`-`9101`.

|TCP/UDP
|`30000`-`32767`
|Kubernetes node port

|===

.All machines to control plane
[cols="2a,2a,5a",options="header"]
|===

|Protocol
|Port
|Description

|TCP
|`6443`
|Kubernetes API

|===

.Control plane machines to control plane machines
[cols="2a,2a,5a",options="header"]
|===

|Protocol
|Port
|Description

|TCP
|`2379`-`2380`
|etcd server and peer ports

|===

[discrete]
== Network topology requirements

The infrastructure that you provision for your cluster must meet the following
network topology requirements.

ifndef::restricted,origin[]
[IMPORTANT]
====
{product-title} requires all nodes to have internet access to pull images
for platform containers and provide telemetry data to Red Hat.
====
endif::restricted,origin[]

[discrete]
=== Load balancers

Before you install {product-title}, you must provision two load balancers that meet the following requirements:

. *API load balancer*: Provides a common endpoint for users, both human and machine, to interact with and configure the platform. Configure the following conditions:
+
--
  ** Layer 4 load balancing only. This can be referred to as Raw TCP, SSL Passthrough, or SSL Bridge mode. If you use SSL Bridge mode, you must enable Server Name Indication (SNI) for the API routes.
  ** A stateless load balancing algorithm. The options vary based on the load balancer implementation.
--
+
[IMPORTANT]
====
Do not configure session persistence for an API load balancer.
====
+
Configure the following ports on both the front and back of the load balancers:
+
.API load balancer
[cols="2,5,^2,^2,2",options="header"]
|===

|Port
|Back-end machines (pool members)
|Internal
|External
|Description

|`6443`
|Bootstrap and control plane. You remove the bootstrap machine from the load
balancer after the bootstrap machine initializes the cluster control plane. You
must configure the `/readyz` endpoint for the API server health check probe.
|X
|X
|Kubernetes API server

|`22623`
|Bootstrap and control plane. You remove the bootstrap machine from the load
balancer after the bootstrap machine initializes the cluster control plane.
|X
|
|Machine config server

|===
+
[NOTE]
====
The load balancer must be configured to take a maximum of 30 seconds from the
time the API server turns off the `/readyz` endpoint to the removal of the API
server instance from the pool. Within the time frame after `/readyz` returns an
error or becomes healthy, the endpoint must have been removed or added. Probing
every 5 or 10 seconds, with two successful requests to become healthy and three
to become unhealthy, are well-tested values.
====

. *Application Ingress load balancer*: Provides an Ingress point for application traffic flowing in from outside the cluster. Configure the following conditions:
+
--
  ** Layer 4 load balancing only. This can be referred to as Raw TCP, SSL Passthrough, or SSL Bridge mode. If you use SSL Bridge mode, you must enable Server Name Indication (SNI) for the Ingress routes.
  ** A connection-based or session-based persistence is recommended, based on the options available and types of applications that will be hosted on the platform.
--
+
Configure the following ports on both the front and back of the load balancers:
+
.Application Ingress load balancer
[cols="2,5,^2,^2,2",options="header"]
|===

|Port
|Back-end machines (pool members)
|Internal
|External
|Description

|`443`
|The machines that run the Ingress router pods, compute, or worker, by default.
|X
|X
|HTTPS traffic

|`80`
|The machines that run the Ingress router pods, compute, or worker, by default.
|X
|X
|HTTP traffic

|===

[TIP]
====
If the true IP address of the client can be seen by the load balancer, enabling source IP-based session persistence can improve performance for applications that use end-to-end TLS encryption.
====

[NOTE]
====
A working configuration for the Ingress router is required for an
{product-title} cluster. You must configure the Ingress router after the control
plane initializes.
====

ifdef::vsphere[]
[discrete]
== Ethernet adaptor hardware address requirements

When provisioning VMs for the cluster, the ethernet interfaces configured for
each VM must use a MAC address from the VMware Organizationally Unique
Identifier (OUI) allocation ranges:

* `00:05:69:00:00:00` to `00:05:69:FF:FF:FF`
* `00:0c:29:00:00:00` to `00:0c:29:FF:FF:FF`
* `00:1c:14:00:00:00` to `00:1c:14:FF:FF:FF`
* `00:50:56:00:00:00` to `00:50:56:FF:FF:FF`

If a MAC address outside the VMware OUI is used, the cluster installation will
not succeed.
endif::vsphere[]

ifdef::vsphere[]
:!vsphere:
endif::[]

ifndef::azure,gcp[]
[discrete]
== NTP configuration

{product-title} clusters are configured to use a public Network Time Protocol (NTP) server by default. If you want to use a local enterprise NTP server, or if your cluster is being deployed in a disconnected network, you can configure the cluster to use a specific time server. For more information, see the documentation for _Configuring chrony time service_.

ifndef::ibm-z[]
If a DHCP server provides NTP server information, the chrony time service on the {op-system-first} machines read the information and can sync the clock with the NTP servers.
endif::ibm-z[]
endif::azure,gcp[]

ifeval::["{context}" == "installing-ibm-z"]
:!ibm-z:
endif::[]
ifeval::["{context}" == "installing-ibm-z-kvm"]
:!ibm-z-kvm:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-z"]
:!restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-ibm-power"]
:!restricted:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-bare-metal"]
:!restricted:
endif::[]
ifeval::["{context}" == "installing-azure-user-infra"]
:!azure:
endif::[]
ifeval::["{context}" == "installing-gcp-user-infra"]
:!gcp:
endif::[]
ifeval::["{context}" == "installing-gcp-user-infra-vpc"]
:!gcp:
endif::[]
ifeval::["{context}" == "installing-restricted-networks-gcp"]
:!gcp:
:!restricted:
endif::[]
