:_mod-docs-content-type: CONCEPT
[id="ols-large-language-model-configuration-overview_{context}"]

= Large Language Model (LLM) configuration overview

You should configure the Large Language Model (LLM) provider you will use prior to installing the {ols-long} Operator.

== About OpenAI configuration with OpenShift Lightspeed

To configure {openai} as the LLM provider to be used with {ols-long}, you must use either your {openai} API key or {openai} project name during the configuration process.

The {openai} service has a feature for projects and service accounts. You may use a service account in a dedicated project so that you can precisely track {ols-long} usage.

For more information, see the official {openai} link:https://platform.openai.com/docs/overview[product documentation].

== About Azure OpenAI configuration with OpenShift Lightspeed

You need a {azure-openai} service instance. There must be at least one model deployment in {azure-studio} for that instance.

For more information, see the official {azure-openai} link:https://learn.microsoft.com/en-us/azure/ai-services/openai/[product documentation].

== About IBM WatsonX configuration with OpenShift Lightspeed

You need an IBM Cloud project with access to {watsonx}. You will also need your {watsonx} API key.

For more information, see the official {watsonx} link:https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/welcome-main.html?context=wx&audience=wdp[product documentation].